Set up Kafka --

Create 2 folders in F drive--
kafka_logs-- zookeeper
kafka_logs-- server_logs

change the zookeeper.properties:
------------------------------------------------------
dataDir=F:/kafka_logs/zookeeper
maxClientCnxns=1

This property limits the number of active connections from a host, specified by IP address, to a single ZooKeeper server.

change the server.properties:
----------------------------------------------------
uncomment listeners
log.dirs=F:/kafka_logs/server_logs
zookeeper.connect=localhost:2181
zookeeper.connection.timeout.ms=60000

Start Zookeeper:
---------------------------------------
F:/kafka_2.13-3.9.1/bin/windows/zookeeper-server-start.bat F:/kafka_2.13-3.9.1/config/zookeeper.properties

Start Kafka-server:
-----------------------------------------
F:/kafka_2.13-3.9.1/bin/windows/kafka-server-start.bat C:/kafka_2.13-3.9.1/config/server.properties

Create topic:
------------------------------------
F:/kafka_2.13-3.9.1/bin/windows/kafka-topics.bat --create --topic {topic_name} --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1

Start Producer:
--------------------------------------
F:/kafka_2.13-3.9.1/bin/windows/kafka-console-producer.bat --topic {topic_name} --bootstrap-server localhost:9092

Start Consumer:
-------------------------------------
F:/kafka_2.13-3.9.1/bin/windows/kafka-console-consumer.bat --topic {topic_name} --from-beginning --bootstrap-server localhost:9092

kafka-python installation:
--------------------------------------------------
pip install kafka-python
(To know more about this client , you can refer this link :
https://pypi.org/project/kafka-python/)

Python Code:
----------------------------------
from time import sleep
from json import dumps
from kafka import KafkaProducer

topic_name='hello_world'
producer = KafkaProducer(bootstrap_servers=['localhost:9092'],value_serializer=lambda x: dumps(x).encode('utf-8'))

for e in range(1000):
    data = {'number' : e}
    print(data)
    producer.send(topic_name, value=data)
    sleep(5)




Kafka Snowflake Integration:
--------------------------------------------------------
Download the required jar file -- https://mvnrepository.com/artifact/com.snowflake/snowflake-kafka-connector/1.5.0

Put this jar in libs folders

Update the plugin.path in kafka connect-standalone properties.

Create Private & Public key-pair:
--------------------------------------------------------------
openssl genrsa -out rsa_key.pem 2048
openssl rsa -in rsa_key.pem -pubout -out rsa_key.pub


Configure the public key in Snowflake:
----------------------------------------------------------------

alter user {User_name} set rsa_public_key='{Put the Public key content here}';

Verify the public key is configured properly or not --
desc user {User_name};



Create a SF_connect.properties file with below properties in config folder --

connector.class=com.snowflake.kafka.connector.SnowflakeSinkConnector
tasks.max=8
topics={topic_name}
snowflake.topic2table.map={topic_name}:{snowflake_table_name}
buffer.count.records=10000
buffer.flush.time=60
buffer.size.bytes=5000000
snowflake.url.name={Snowflake URL}
snowflake.user.name={Snowflake User Name}
snowflake.private.key={Put the Private key content here}
snowflake.database.name={Snowflake Database Name}
snowflake.schema.name={Snowflake Schema Name}
key.converter=com.snowflake.kafka.connector.records.SnowflakeJsonConverter
value.converter=com.snowflake.kafka.connector.records.SnowflakeJsonConverter
name={}

Create the topic if not already exists & run the python code to ingest the data in the topic.


Start the Kafka Connector:
---------------------------------------------------------
F:/kafka_2.13-3.9.1/bin/windows/connect-standalone.bat 
F:/kafka_2.13-3.9.1/config/connect-standalone.properties
F:/kafka_2.13-3.9.1/config/SF_connect.properties

To unset the Public Key in Snowflake:
----------------------------------------------------------------------
alter user {User_name} unset rsa_public_key;